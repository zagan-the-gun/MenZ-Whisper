#!/usr/bin/env python3
"""
GPUç’°å¢ƒãƒã‚§ãƒƒã‚¯ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
MenZ-Whisperã§åˆ©ç”¨å¯èƒ½ãªGPUç’°å¢ƒã‚’ç¢ºèªã—ã¾ã™ã€‚
"""

import sys
import platform
from pathlib import Path

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))


def check_python_environment():
    """Pythonç’°å¢ƒãƒã‚§ãƒƒã‚¯"""
    print("ğŸ Pythonç’°å¢ƒ:")
    print(f"  ãƒãƒ¼ã‚¸ãƒ§ãƒ³: {sys.version}")
    print(f"  ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ : {platform.platform()}")
    print()


def check_pytorch():
    """PyTorchç’°å¢ƒãƒã‚§ãƒƒã‚¯"""
    print("ğŸ”¥ PyTorchç’°å¢ƒ:")
    try:
        import torch
        print(f"  PyTorch: {torch.__version__}")
        print(f"  CUDAåˆ©ç”¨å¯èƒ½: {torch.cuda.is_available()}")
        
        if torch.cuda.is_available():
            print(f"  CUDAãƒãƒ¼ã‚¸ãƒ§ãƒ³: {torch.version.cuda}")
            gpu_count = torch.cuda.device_count()
            print(f"  GPUãƒ‡ãƒã‚¤ã‚¹æ•°: {gpu_count}")
            print()
            
            print("  åˆ©ç”¨å¯èƒ½ãªGPU:")
            for i in range(gpu_count):
                gpu_name = torch.cuda.get_device_name(i)
                gpu_properties = torch.cuda.get_device_properties(i)
                gpu_memory = gpu_properties.total_memory / (1024**3)
                compute_capability = f"{gpu_properties.major}.{gpu_properties.minor}"
                
                # ãƒ¡ãƒ¢ãƒªä½¿ç”¨çŠ¶æ³
                try:
                    torch.cuda.empty_cache()
                    allocated = torch.cuda.memory_allocated(i) / (1024**3)
                    cached = torch.cuda.memory_reserved(i) / (1024**3)
                    free = gpu_memory - cached
                    print(f"    GPU {i}: {gpu_name}")
                    print(f"      ãƒ¡ãƒ¢ãƒª: {gpu_memory:.1f}GB (ä½¿ç”¨ä¸­: {cached:.1f}GB, ç©ºã: {free:.1f}GB)")
                    print(f"      Compute Capability: {compute_capability}")
                except Exception:
                    print(f"    GPU {i}: {gpu_name}")
                    print(f"      ãƒ¡ãƒ¢ãƒª: {gpu_memory:.1f}GB")
                    print(f"      Compute Capability: {compute_capability}")
                print()
            
            if gpu_count > 1:
                print("  ğŸ” è¤‡æ•°GPUãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸï¼")
                print("  è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã§GPU IDã‚’æŒ‡å®šã™ã‚‹ã“ã¨ã§ç‰¹å®šã®GPUã‚’ä½¿ç”¨ã§ãã¾ã™:")
                print("  config.ini ã® [inference] ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§")
                print("    device = cuda")
                print("    gpu_id = 0 # ä½¿ç”¨ã—ãŸã„GPUã®IDï¼ˆ0ã‹ã‚‰å§‹ã¾ã‚‹ï¼‰")
                print()
        
        # Apple Silicon (MPS) ãƒã‚§ãƒƒã‚¯
        if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            print("  MPSï¼ˆApple Siliconï¼‰: åˆ©ç”¨å¯èƒ½")
            print("  âš ï¸ faster-whisperã¯MPSéå¯¾å¿œã®ãŸã‚ã€use_faster_whisper=trueã®å ´åˆã¯CPUãŒä½¿ç”¨ã•ã‚Œã¾ã™")
            print()
            
    except ImportError:
        print("  âŒ PyTorchãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        print("  pip install torch ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„")
        print()


def check_whisper():
    """Whisperç’°å¢ƒãƒã‚§ãƒƒã‚¯"""
    print("ğŸ¤ Whisperç’°å¢ƒ:")
    
    # OpenAI Whisper
    try:
        import whisper
        print(f"  openai-whisper: ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿")
        available_models = whisper.available_models()
        print(f"  åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«: {', '.join(available_models)}")
    except ImportError:
        print("  âŒ openai-whisperãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        print("  pip install openai-whisper ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„")
    
    # Faster Whisper
    try:
        import faster_whisper
        print(f"  faster-whisper: ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿")
        print(f"  ãƒãƒ¼ã‚¸ãƒ§ãƒ³: {faster_whisper.__version__}")
        print("  âœ… æ¨å¥¨: faster-whisperã¯æ¨™æº–ç‰ˆã‚ˆã‚Šé«˜é€Ÿã§ã™")
    except ImportError:
        print("  âŒ faster-whisperãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ï¼ˆæ¨å¥¨ï¼‰")
        print("  pip install faster-whisper ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„")
    
    print()


def check_audio_libraries():
    """éŸ³å£°å‡¦ç†ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãƒã‚§ãƒƒã‚¯"""
    print("ğŸµ éŸ³å£°å‡¦ç†ãƒ©ã‚¤ãƒ–ãƒ©ãƒª:")
    
    # numpy
    try:
        import numpy as np
        print(f"  numpy: {np.__version__}")
    except ImportError:
        print("  âŒ numpyãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        print("  pip install numpy ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„")
    
    # soundfile
    try:
        import soundfile
        print(f"  soundfile: {soundfile.__version__}")
    except ImportError:
        print("  âŒ soundfileãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        print("  pip install soundfile ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„")
    
    # librosa
    try:
        import librosa
        print(f"  librosa: {librosa.__version__}")
    except ImportError:
        print("  âŒ librosaãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        print("  pip install librosa ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„")
    
    # silero-vad
    try:
        import silero_vad
        print(f"  silero-vad: ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿")
    except ImportError:
        print("  âŒ silero-vadãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        print("  pip install silero-vad ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„")
    
    print()


def check_websocket():
    """WebSocketç’°å¢ƒãƒã‚§ãƒƒã‚¯"""
    print("ğŸ“¡ WebSocketç’°å¢ƒ:")
    try:
        import websockets
        print(f"  websockets: {websockets.__version__}")
    except ImportError:
        print("  âŒ websocketsãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        print("  pip install websockets ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„")
    
    print()


def check_system_resources():
    """ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ãƒã‚§ãƒƒã‚¯"""
    print("ğŸ’» ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹:")
    try:
        import psutil
        
        # CPUæƒ…å ±
        cpu_count = psutil.cpu_count()
        cpu_percent = psutil.cpu_percent(interval=1)
        print(f"  CPU: {cpu_count}ã‚³ã‚¢ (ä½¿ç”¨ç‡: {cpu_percent}%)")
        
        # ãƒ¡ãƒ¢ãƒªæƒ…å ±
        memory = psutil.virtual_memory()
        memory_gb = memory.total / (1024**3)
        memory_percent = memory.percent
        print(f"  ãƒ¡ãƒ¢ãƒª: {memory_gb:.1f}GB (ä½¿ç”¨ç‡: {memory_percent}%)")
        
        if memory_gb < 4:
            print("  âš ï¸ ãƒ¡ãƒ¢ãƒªãŒ4GBæœªæº€ã§ã™ã€‚å‹•ä½œãŒåˆ¶é™ã•ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
        elif memory_gb < 8:
            print("  âš ï¸ ãƒ¡ãƒ¢ãƒªãŒ8GBæœªæº€ã§ã™ã€‚å¤§ããªãƒ¢ãƒ‡ãƒ«ã§å•é¡ŒãŒç”Ÿã˜ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
        
        # ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡
        disk = psutil.disk_usage('.')
        disk_gb = disk.free / (1024**3)
        print(f"  åˆ©ç”¨å¯èƒ½ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡: {disk_gb:.1f}GB")
        
        if disk_gb < 5:
            print("  âš ï¸ ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«å¿…è¦ã§ã™ã€‚")
        
        print()
        
    except ImportError:
        print("  psutilãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰")
        print("  pip install psutil ã§ã‚ˆã‚Šè©³ç´°ãªæƒ…å ±ã‚’ç¢ºèªã§ãã¾ã™")
        print()


def check_config():
    """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒã‚§ãƒƒã‚¯"""
    print("âš™ï¸ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒã‚§ãƒƒã‚¯:")
    
    config_path = project_root / "config.ini"
    if config_path.exists():
        print(f"  âœ… config.ini ãŒå­˜åœ¨ã—ã¾ã™")
        
        try:
            from app.config import WhisperConfig
            config = WhisperConfig.from_ini(str(config_path))
            print(f"  ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º: {config.model_size}")
            print(f"  faster-whisperä½¿ç”¨: {config.use_faster_whisper}")
            print(f"  ãƒ‡ãƒã‚¤ã‚¹è¨­å®š: {config.device}")
            print(f"  GPU ID: {config.gpu_id}")
            print(f"  è¨€èª: {config.language or 'auto'}")
        except Exception as e:
            print(f"  âš ï¸ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    else:
        print(f"  âš ï¸ config.ini ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        print(f"  ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒåŒæ¢±ã•ã‚Œã¦ã„ã‚‹ã¯ãšã§ã™")
    
    print()


def test_whisper_model():
    """Whisperãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆ"""
    print("ğŸ§ª ç·åˆå‹•ä½œãƒ†ã‚¹ãƒˆ:")
    
    try:
        from app.config import WhisperConfig
        from app.model import WhisperModel
        import numpy as np
        
        print("  è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ä¸­...")
        config = WhisperConfig()
        
        # ãƒ†ã‚¹ãƒˆç”¨ã«å°ã•ã„ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
        original_model_size = config.model_size
        if config.model_size not in ['tiny', 'base']:
            config.model_size = 'tiny'
            print(f"  ãƒ†ã‚¹ãƒˆç”¨ã« {config.model_size} ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™")
        
        print("  Whisperãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ä¸­...")
        print("  (åˆå›å®Ÿè¡Œæ™‚ã¯ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãŒè¡Œã‚ã‚Œã‚‹ãŸã‚æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™...)")
        
        model = WhisperModel(config)
        print("  âœ… ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–æˆåŠŸ")
        
        # ãƒ€ãƒŸãƒ¼éŸ³å£°ã§ãƒ†ã‚¹ãƒˆï¼ˆ1ç§’ã®ç„¡éŸ³ï¼‰
        print("  éŸ³å£°èªè­˜ãƒ†ã‚¹ãƒˆä¸­...")
        dummy_audio = np.zeros(16000, dtype=np.float32)
        result = model.transcribe_audio_segment(dummy_audio)
        print(f"  âœ… éŸ³å£°èªè­˜ãƒ†ã‚¹ãƒˆæˆåŠŸ (çµæœ: '{result}')")
        
        # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        model.cleanup()
        print("  âœ… ç·åˆãƒ†ã‚¹ãƒˆæˆåŠŸ")
        
    except Exception as e:
        print(f"  âŒ ç·åˆãƒ†ã‚¹ãƒˆå¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
    
    print()


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    print("=" * 50)
    print("ğŸ” MenZ-Whisper ç’°å¢ƒãƒã‚§ãƒƒã‚¯")
    print("=" * 50)
    print()
    
    check_python_environment()
    check_pytorch()
    check_whisper()
    check_audio_libraries()
    check_websocket()
    check_system_resources()
    check_config()
    test_whisper_model()
    
    print("=" * 50)
    print("âœ… ç’°å¢ƒãƒã‚§ãƒƒã‚¯å®Œäº†")
    print("=" * 50)
    print()
    print("æ¨å¥¨è¨­å®š:")
    print("  - GPUãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆ: device = cuda, use_faster_whisper = true")
    print("  - Apple Siliconã®å ´åˆ: device = mps, use_faster_whisper = false")
    print("  - CPUã®ã¿ã®å ´åˆ: device = cpu, use_faster_whisper = true")
    print()


if __name__ == "__main__":
    main()


